DLA-Future implements an efficient GPU-enabled distributed eigenvalue solver using a software architecture based on the latest C++ std::execution concurrency proposals.
The state-of-the-art linear algebra implementations LAPACK and ScaLAPACK were designed for legacy systems and employ the so-called fork join parallelism, which can perform inefficiently on modern architectures.
The benefits of task-based linear algebra implementations are significant. The reduction of the number of synchronization points and the ease of overlapping computation with communication are two of the main benefits that lead to improved performance. In specific cases (which depend on the problem sizes, dependencies and number of independent operations), the ability to schedule multiple algorithms concurrently yields a noticeable reduction of time-to-solution.

We present the implementation of DLA-Future and the results on different types of systems starting from Piz Daint multicore and GPU partitions (Intel Broadwell/Haswell, Nvidia P100 GPUs), moving to more recent architectures available in ALPS (AMD Zen 2 CPUs, Nvidia A100 GPUs) and LUMI (AMD MI250x GPUs).
The benchmark results are divided into two categories. The first contains a comparison of DLA-Future against widely used eigensolver implementations. The second category showcases the performance of the eigensolver in real applications. We present results generated with CP2K and SIRIUS, where DLA-future support was easily added thanks to the C-API provided, which is drop-in compatible with the ScaLAPACK interface. 
