Python has seen significant adoption by the scientific computing community in recent years. Libraries such as Numpy and SciPy have made it possible to prototype complex physical systems with decent performance and low programming effort. In particular, computations on structured grids that are at the heart of several numerical methods such as finite difference, finite volume, geometric multigrid, etc. are highly regular and have static dependence patterns which can be expressed succinctly using Numpy's slicing notations. Numpy execution is limited to a single thread and can occasionally make calls to high-performance multi-threaded BLAS libraries for certain in-built functions. However, for any arbitrary computations on structured grids, Numpy suffers from significant Python overheads and can potentially create several temporary arrays in the process. Moreover, Numpy doesn't scale to distributed memory machines. Thus, domain scientists often have to reimplement the application in a low-level language after prototyping in Python to get good parallel performance. There have been several libraries developed for distributed array computations in Python to address this issue. These are usually developed as either a compiler for a subset of Python or a drop-in replacement for Numpy. However, these libraries either require a significant rewrite of the source code or often show poor strong scaling performance due to high Python overheads. In this paper, we present CharmStencil, a high-level abstraction for expressing computations on structured grids with a Numpy-like Python frontend and a Charm++ backend. CharmStencil is a part of the broader CharmTyles project which aims to build multiple domain-specific abstractions, with a high-level frontend, combined with a parallel domain-specific library at the backend. We present a client-server model that we use to build our abstraction to preserve the productivity offered by tools such as Jupyter notebooks on the frontend, while the actual computations are executed by a highly efficient Charm++ backend. The frontend employs optimizations such as lazy evaluation and message coalescing to avoid the creation of temporary arrays and minimize communication latency between the frontend and the backend. The client-server model with an asynchronous frontend is able to provide a pipelined execution by overlapping the Python overhead incurred by the frontend with useful computation on the backend, thus making our abstraction highly scalable. Finally, we compare our performance against state-of-the-art distributed array computation libraries and compilers for Python.
